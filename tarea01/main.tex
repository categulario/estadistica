\documentclass[11pt,letterpaper, reqno]{article}

\textwidth      =  6in
\textheight     =  8.25in
\oddsidemargin  =  18pt
\evensidemargin =  18pt
\topmargin      =  0.00in

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{{img/}}

\title{Análisis de dos Clasificadores}
\author{Abraham Toriz\\ Jesús Mejía\\ Luis Cortés\\ Roberto Saucedo}

\newtheorem{definition}{Definición}[section]

\begin{document}
\pagestyle{empty}
\begin{minipage}{.15\textwidth}
	\includegraphics[scale=.22]{uv.eps}
\end{minipage}
\begin{minipage}{.7\textwidth}
\centering
{\huge \sc Universidad Veracruzana}\\
{\LARGE \sc Facultad de Matemáticas}
\end{minipage}
\begin{minipage}{.15\textwidth}
	\begin{flushright}
		\includegraphics[scale=.22]{matematicas.eps}
	\end{flushright}
\end{minipage}

\vspace{.7cm}
\begin{center}
{\huge \bf
	Análisis de dos Clasificadores
}
\\[1.5cm]
{ \Huge
	T A R E A
}
\\[1.5cm]

{ \large
	Que para pasar la materia
}
\\[.5cm]
{ \LARGE \bf
	Temas Selectos de Estadística
}
\\[1cm]
Presentan los siguientes\\[1cm]
{ \Large
I N T E G R A N T E S:
}\\
{ \LARGE \bf
	Abraham Toriz\\
	Jesús Mejía\\
	Luis Cortés\\
	Roberto Saucedo
}\\[1cm]
{ \Large
CATEDRÁTICO:
}\\
{ \LARGE \bf
	Martha Lorena Avendaño
}
\end{center}
\vspace{1.5cm}
\begin{minipage}{.5\textwidth}
\centering
Xalapa, Veracruz
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
4 de septiembre de 2015
\end{minipage}

%-----------------%
\pagebreak
\pagestyle{plain}

\begin{abstract}
En el afán del ser humano por reducir el esfuerzo para concretar una tarea se han desarrollado técnicas y conocimientos que suplen a la intuicíón. Una de estas tareas es la clasificación de objetos para tomar mejores decisiones. Con tal fin en la matemática, por medio de la estadística, se tienen métodos que permiten aproximar un modelo de clasificación de datos basados en una muestra. En este proyecto estudiamos dos de ellos: el modelo de regresión logística y el modelo de los $n$ vecinos más cercanos.
\end{abstract}

\section{Consideraciones}

Para poder clasificar los datos por medio de un algoritmo es necesario hacer \textit{suposiciones} acerca de la información que tenemos de entrada. Una de las suposiciones necesarias es pensar que existe algún modelo $f$ (aunque no lo conozcamos) que nos proporciona la clasificación de los datos.

$$
Y = f(X) + \epsilon
$$

donde $X$ es la información de entrada, $Y$ la clase a la que pertenece y $\epsilon$  una variable aleatoria independiente de $X$ con media cero que se relaciona con el error introducido por el medio.

Con esta información tratamos de construir una estimación del modelo $f$ que llamamos $\hat{f}$ con la cual tratamos de aproximar $Y$. A esta aproximación le llamamos $\hat{Y}$.

$$
\hat{Y} = \hat{f}(X)
$$

\subsection{Tipos de modelos}

Trabajaremos con dos tipos de modelos según la forma en que estos se construyen a partir de los datos muestra.

\begin{itemize}
	\item \textbf{Modelos paramétricos} Son aquellos que dependen de parámetros y estos parámetros se ajustan bajo algún criterio del \textit{mejor ajuste} a partir de los datos muestra. Se espera que estos datos sean suficientemente buenos (y los parámetros bien escogidos) para que el modelo clasifique bien datos que no están en la muestra.
	\item \textbf{Modelos no paramétricos} No dependen de parámetros sin embargo se ajusta el clasificador a partir de una muestra representativa de los datos a clasificar.
\end{itemize}

\section{Regresión Logística}

¿Cómo podríamos modelar la relación ente $p(X) = P(Y=1|X)$ y $X$? Pues bien, sabemos que podemos utilizar un modelo de regresión lineal para representar dichas probabilidades:
\begin{equation*}
p(X) = \beta_0 + \beta_1X.
\end{equation*}

Sin embargo nos gustaría una función $p$ tal que modele la probabilidad, es decir, que tenga imágenes entre 0 y 1 para todos los valores de $X$. En la regresión logística, se usa la función \textit{logística},

\begin{equation}
	\label{eqn:func_logistica}
	p(X) = \frac{e^{\beta_0 + \beta_1^{T} X}}{1+e^{\beta_0 + \beta_1^{T}X}},
\end{equation}
donde:
\begin{itemize}
	\item[$\beta_0$]$\in \mathbb{R}$
	\item[$\beta_1^T$] es un vector con $p$ entradas.
\end{itemize}
Para ajustar el modelo (\ref{eqn:func_logistica}), usamos el método de la máxima verosimilitud. Esta técnica propone una función paramétrica:
\begin{equation}
	\label{eqn:func_L}
	\mathcal{L}(\beta_{0}, \beta_{1}) = \prod_{i=1}^{N}p(X_i)^{Y_i}[1-p(X_{i})]^{1-Y_i}
\end{equation}
Los estimadores $\hat{\beta_0}$ y $\hat{\beta_1}$ se eligen de tal que manera que maximicen la función $\mathcal{L}$ (conocida como función de verosimilitud). Las máxima verosimilitud es una aproximación general que se usa para ajustar modelos no lineales.\\

Si sustituimos la ecuación (\ref{eqn:func_logistica}) en (\ref{eqn:func_L}) y desarrollando obtenemos,
$$
\mathcal{L}(\beta_{0}, \beta_{1}) =  \prod_{i=1}^{N} \left[ \frac{e^{\beta_0 + \beta_1^{T} X}}{1+e^{\beta_0 + \beta_1^{T}X}} \right]^{Y_i} \left[\frac{1}{1+e^{\beta_0 + \beta_1^{T}X}} \right]^{1-Y_i}.
$$
Aplicando el logaritmo, tenemos
$$
\ell(\beta_{0}, \beta_{1}) = \log\mathcal{L}(\beta_{0}, \beta_{1}) = \sum_{i=1}^{N} Y_i\left[ \frac{e^{\beta_0 + \beta_1^{T} X}}{1+e^{\beta_0 + \beta_1^{T}X}} \right] +  (1-Y_i)\left[\frac{1}{1+e^{\beta_0 + \beta_1^{T}X}} \right].
$$
Ahora, solo resta maximizar dicha función, a través métodos numéricos apropiados.

\section{Los $k$ Vecinos más Cercanos}

La idea básica sobre la que se fundamenta este clasificador es que una nueva muestra será clasificada a la clase más frecuente a la que pertenecen sus vecinos más cercanos.

$k$-NN es un algoritmo perezoso, esto es, que durante el entrenamiento, solo guarda instancias y no contruye ningun modelo, tampoco hace supuestos sobre la distribucion que siguen los datos, por lo tanto es no parametrico.

Dado un conjunto de datos $ X= \left\lbrace X_1,X_2, ... , X_p \right\rbrace $ y un conjunto de clases $ C= \left\lbrace C_1,C_2, ... , C_m \right\rbrace $, el  problema de clasificación es encontrar una función $ f:X \longrightarrow C $ tal que cada $ X_i $ es asignada a una clase $ C_j $.

La fase de entrenamiento del algoritmo consiste en almacenar los vectores característicos y las etiquetas de las clases de los ejemplos de entrenamiento.  En la fase de clasificación, la evaluación del ejemplo (del que no se conoce su clase) es representada por un vector en el espacio característico.
$ x = \left\lbrace x_1,x_2, ... , x_m \right\rbrace $
Se calcula la distancia entre los vectores almacenados y el nuevo vector, generalmente se usa la distancia euclidiana

\begin{center}
$ d(x_i,x_j)=\sqrt{\Sigma_{r=1}^{p} (x_{ri}-x_{rj})^2} $
\end{center}
y se seleccionan los $ k $ ejemplos más cercanos. 

El nuevo ejemplo es clasificado con la clase que más se repite en los vectores seleccionados. Este método supone que los vecinos más cercanos nos dan la mejor clasificación y esto se hace utilizando todos los atributos. Los valores de los atributos del i-esimo ejemplo con $ i \epsilon \left\lbrace 1, .. , n \right\rbrace $ se representan por el vector p-dimensional

El problema de dicha suposición es que es posible que se tengan muchos atributos irrelevantes que dominen sobre la clasificación: dos atributos relevantes perderían peso entre otros veinte irrelevantes.

Para corregir el posible sesgo se puede asignar un peso a las distancias de cada atributo, dándole así mayor importancia a los atributos más relevantes. Otra posibilidad consiste en tratar de determinar o ajustar los pesos con ejemplos conocidos de entrenamiento. Finalmente, antes de asignar pesos es recomendable identificar y eliminar los atributos que se consideran irrelevantes.

La mejor elección de $ k $ depende fundamentalmente de los datos; generalmente, valores grandes de $ k $ reducen el efecto de ruido en la clasificación, pero crean límites entre clases parecidas. Un buen $ k $ puede ser seleccionado mediante una optimización de uso.

La exactitud de este algoritmo puede ser severamente degradada por la presencia de ruido o características irrelevantes, o si las escalas de características no son consistentes con lo que uno considera importante. 

\section{Observaciones}

\end{document}
