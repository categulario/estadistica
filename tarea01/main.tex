\documentclass[11pt,letterpaper, reqno]{article}

\textwidth      =  6in
\textheight     =  8.25in
\oddsidemargin  =  18pt
\evensidemargin =  18pt
\topmargin      =  0.00in

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{{img/}}

\title{Análisis de dos Clasificadores}
\author{Abraham Toriz\\ Jesús Mejía\\ Luis Cortés\\ Roberto Saucedo}

\newtheorem{definition}{Definición}[section]

\begin{document}
\pagestyle{empty}
\begin{minipage}{.15\textwidth}
	\includegraphics[scale=.22]{uv.eps}
\end{minipage}
\begin{minipage}{.7\textwidth}
\centering
{\huge \sc Universidad Veracruzana}\\
{\LARGE \sc Facultad de Matemáticas}
\end{minipage}
\begin{minipage}{.15\textwidth}
	\begin{flushright}
		\includegraphics[scale=.22]{matematicas.eps}
	\end{flushright}
\end{minipage}

\vspace{.7cm}
\begin{center}
{\huge \bf
	Análisis de dos Clasificadores
}
\\[1.5cm]
{ \Huge
	T A R E A
}
\\[1.5cm]

{ \large
	Que para pasar la materia
}
\\[.5cm]
{ \LARGE \bf
	Temas Selectos de Estadística
}
\\[1cm]
Presentan los siguientes\\[1cm]
{ \Large
I N T E G R A N T E S:
}\\
{ \LARGE \bf
	Abraham Toriz\\
	Jesús Mejía\\
	Luis Cortés\\
	Roberto Saucedo
}\\[1cm]
{ \Large
CATEDRÁTICO:
}\\
{ \LARGE \bf
	Martha Lorena Avendaño
}
\end{center}
\vspace{1.5cm}
\begin{minipage}{.5\textwidth}
\centering
Xalapa, Veracruz
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
4 de septiembre de 2015
\end{minipage}

%-----------------%
\pagebreak
\pagestyle{plain}

\begin{abstract}
En el afán del ser humano por reducir el esfuerzo para concretar una tarea se han desarrollado técnicas y conocimientos que suplen a la intuicíón. Una de estas tareas es la clasificación de objetos para tomar mejores decisiones. Con tal fin en la matemática, por medio de la estadística, se tienen métodos que permiten aproximar un modelo de clasificación de datos basados en una muestra. En este proyecto estudiamos dos de ellos: el modelo de regresión logística y el modelo de los $n$ vecinos más cercanos.
\end{abstract}

\section{Consideraciones}

Para poder clasificar los datos por medio de un algoritmo es necesario hacer \textit{suposiciones} acerca de la información que tenemos de entrada. Una de las suposiciones necesarias es pensar que existe algún modelo $f$ (aunque no lo conozcamos) que nos proporciona la clasificación de los datos.

$$
Y = f(X) + \epsilon
$$

donde $X$ es la información de entrada, $Y$ la clase a la que pertenece y $\epsilon$  una variable aleatoria independiente de $X$ con media cero que se relaciona con el error introducido por el medio.

Con esta información tratamos de construir una estimación del modelo $f$ que llamamos $\hat{f}$ con la cual tratamos de aproximar $Y$. A esta aproximación le llamamos $\hat{Y}$.

$$
\hat{Y} = \hat{f}(X)
$$

\subsection{Tipos de modelos}

Trabajaremos con dos tipos de modelos según la forma en que estos se construyen a partir de los datos muestra.

\begin{itemize}
	\item \textbf{Modelos paramétricos} Son aquellos que dependen de parámetros y estos parámetros se ajustan bajo algún criterio del \textit{mejor ajuste} a partir de los datos muestra. Se espera que estos datos sean suficientemente buenos (y los parámetros bien escogidos) para que el modelo clasifique bien datos que no están en la muestra.
	\item \textbf{Modelos no paramétricos} No dependen de parámetros sin embargo se ajusta el clasificador a partir de una muestra representativa de los datos a clasificar.
\end{itemize}

\section{Regresión Logística}

¿Cómo podríamos modelar la relación ente $p(X) = P(Y=1|X)$ y $X$? Pues bien, sabemos que podemos utilizar un modelo de regresión lineal para representar dichas probabilidades:
\begin{equation*}
p(X) = \beta_0 + \beta_1X.
\end{equation*}

Sin embargo nos gustaría una función $p$ tal que modele la probabilidad, es decir, que tenga imágenes entre 0 y 1 para todos los valores de $X$. En la regresión logística, se usa la función \textit{logística},

\begin{equation}
	\label{eqn:func_logistica}
	p(X) = \frac{e^{\beta_0 + \beta_1^{T} X}}{1+e^{\beta_0 + \beta_1^{T}X}},
\end{equation}
donde:
\begin{itemize}
	\item[$\beta_0$]$\in \mathbb{R}$
	\item[$\beta_1^T$] es un vector con $p$ entradas.
\end{itemize}
Para ajustar el modelo (\ref{eqn:func_logistica}), usamos el método de la máxima verosimilitud. Esta técnica propone una función paramétrica:
\begin{equation}
	\label{eqn:func_L}
	\mathcal{L}(\beta_{0}, \beta_{1}) = \prod_{i=1}^{N}p(X_i)^{Y_i}[1-p(X_{i})]^{1-Y_i}
\end{equation}
Los estimadores $\hat{\beta_0}$ y $\hat{\beta_1}$ se eligen de tal que manera que maximicen la función $\mathcal{L}$ (conocida como función de verosimilitud). Las máxima verosimilitud es una aproximación general que se usa para ajustar modelos no lineales.\\

Si sustituimos la ecuación (\ref{eqn:func_logistica}) en (\ref{eqn:func_L}) y desarrollando obtenemos,
$$
\mathcal{L}(\beta_{0}, \beta_{1}) =  \prod_{i=1}^{N} \left[ \frac{e^{\beta_0 + \beta_1^{T} X}}{1+e^{\beta_0 + \beta_1^{T}X}} \right]^{Y_i} \left[\frac{1}{1+e^{\beta_0 + \beta_1^{T}X}} \right]^{1-Y_i}.
$$
Aplicando el logaritmo, tenemos
$$
\ell(\beta_{0}, \beta_{1}) = \log\mathcal{L}(\beta_{0}, \beta_{1}) = \sum_{i=1}^{N} Y_i\left[ \frac{e^{\beta_0 + \beta_1^{T} X}}{1+e^{\beta_0 + \beta_1^{T}X}} \right] +  (1-Y_i)\left[\frac{1}{1+e^{\beta_0 + \beta_1^{T}X}} \right].
$$
Ahora, solo resta maximizar dicha función, a través métodos numéricos apropiados.
\section{Los $k$ Vecinos más Cercanos}
En este apartado se describirá el modelo de interés...

\section{Observaciones}

\end{document}
